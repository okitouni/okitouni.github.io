<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts | Ouail Kitouni</title><link>https://okitouni.github.io/post/</link><atom:link href="https://okitouni.github.io/post/index.xml" rel="self" type="application/rss+xml"/><description>Posts</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Tue, 21 Sep 2021 00:00:00 +0000</lastBuildDate><image><url>https://okitouni.github.io/images/icon_hu78093045f097574c5772539a9c4bdc89_10083_512x512_fill_lanczos_center_3.png</url><title>Posts</title><link>https://okitouni.github.io/post/</link></image><item><title>Some thoughts on Grokking</title><link>https://okitouni.github.io/post/grokking-thoughts/</link><pubDate>Tue, 21 Sep 2021 00:00:00 +0000</pubDate><guid>https://okitouni.github.io/post/grokking-thoughts/</guid><description>&lt;p>This page is a work-in-progress.&lt;/p>
&lt;h2 id="animations">Animations&lt;/h2>
&lt;p>The example below is an animation of the first Figure from &lt;a href="https://okitouni.github.io/publication/grok/">our Grokking paper&lt;/a>.
We visualize the first two principal components of the embeddings of a transformer model trained on the task of addition modulo 59. An interesting feature is that generalization seems to coincide with learnning an ordered representation of the embeddings around a circle (much like how a human would reason about modular addition).
&lt;video controls >
&lt;source src="modular-addition59-deep_pca12.mp4" type="video/mp4">
&lt;/video>
What is more intriguing is that this &amp;ldquo;perfect&amp;rdquo; representation actually exists very early (up to local ordering of embeddings). In the animation below, the axes are fixed to the first two principal components &lt;strong>at the end of training&lt;/strong>. This seems to suggest that the network picks a good representation at initialization and prunes away the noise throughout training. As one might expect, this &amp;ldquo;lottery ticket&amp;rdquo; found at initialization gets better (&lt;em>i.e.&lt;/em>, closer to the perfect representation) the wider the model dimension.
&lt;video controls >
&lt;source src="modular-addition59-deep_pca12_fixed.mp4" type="video/mp4">
&lt;/video>
&lt;em>N.b.&lt;/em>, it is not necessary to get a perfect ordering in the first two principal components for generalization to occur. In some cases, the circle can be &amp;ldquo;spread out&amp;rdquo; over many components at different frequencies. For instance, the circle can be ordered modulo 2 in the first two principal components such that there are two degenerate cicles on top of each other. But this multiplicity is broken in lower principal components.&lt;/p></description></item></channel></rss>