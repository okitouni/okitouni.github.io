[{"authors":null,"categories":null,"content":"I am a fourth year Ph.D. student at the Lab for Nuclear Science and the Statistics and Data Science Center at MIT. As a Junior Investigator at the Institute for AI and Fundamental Interactions, I am mainly interested in research at the interface between machine learning and physics. Problems I work on include AI robustness, fairness, and interpretability applied to high energy physics. I am also interested in understanding the science of deep learning and AI foundations.\n","date":1668729600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1668729600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://okitouni.github.io/author/ouail-kitouni/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/ouail-kitouni/","section":"authors","summary":"I am a fourth year Ph.D. student at the Lab for Nuclear Science and the Statistics and Data Science Center at MIT. As a Junior Investigator at the Institute for AI and Fundamental Interactions, I am mainly interested in research at the interface between machine learning and physics.","tags":null,"title":"Ouail Kitouni","type":"authors"},{"authors":["Ouail Kitouni"],"categories":[],"content":" This page is a work-in-progress.\nNeural Estimation Energy Mover\u0026rsquo;s Distance (NEEMo) Triangle and Ellipse Fit Fitting the target distribution (green) with an ellipse and a triangle (red) using the NEEMo algorithm.\nMulti-circle Fit Now we fit the target distribution (green) with three circles (red). ","date":1668729600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668729600,"objectID":"c688edf0ac35fbf1048527274d94a63d","permalink":"https://okitouni.github.io/post/animations/","publishdate":"2022-11-18T00:00:00Z","relpermalink":"/post/animations/","section":"post","summary":"A collection of visualization I made in various projects","tags":[],"title":"Random Animations and Visualizations","type":"post"},{"authors":["Ouail Kitouni","Niklas Nolte","Mike Williams"],"categories":null,"content":" ","date":1663718400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1663718400,"objectID":"c6d8f170819df0005318e403a521a22f","permalink":"https://okitouni.github.io/publication/neemo/","publishdate":"2022-09-21T00:00:00Z","relpermalink":"/publication/neemo/","section":"publication","summary":"NEEMo is a technique to fit arbitrary geometries to an arbitrary collection of points. Much like WGANs, it relies on the neural estimation of the Wasserstein metric through the KR dual formulation.","tags":["Deep Learning","Particle Physics"],"title":"NEEMo: Geometric Fitting using Neural Estimation of the Energy Mover's Distance","type":"publication"},{"authors":["Ouail Kitouni"],"categories":[],"content":" This page is a work-in-progress.\nSummary Animations The example below is an animation of the first Figure from our Grokking paper. We visualize the first two principal components of the embeddings of a transformer model trained on the task of addition modulo 59. An interesting feature is that generalization seems to coincide with learnning an ordered representation of the embeddings around a circle (much like how a human would reason about modular addition). What is more intriguing is that this \u0026ldquo;perfect\u0026rdquo; representation actually exists very early (up to local ordering of embeddings). In the animation below, the axes are fixed to the first two principal components at the end of training. This seems to suggest that the network picks a good representation at initialization and prunes away the noise throughout training. As one might expect, this \u0026ldquo;lottery ticket\u0026rdquo; found at initialization gets better (i.e., closer to the perfect representation) the wider the model dimension. N.b., it is not necessary to get a perfect ordering in the first two principal components for generalization to occur. In some cases, the circle can be \u0026ldquo;spread out\u0026rdquo; over many components at different frequencies. For instance, the circle can be ordered modulo 2 in the first two principal components such that there are two degenerate cicles on top of each other. But this multiplicity is broken in lower principal components.\n","date":1663718400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1663718400,"objectID":"c55acc8b759e9962841e40246aca1fc1","permalink":"https://okitouni.github.io/post/grokking-thoughts/","publishdate":"2022-09-21T00:00:00Z","relpermalink":"/post/grokking-thoughts/","section":"post","summary":"A short blog post on \"Towards Understanding Grokking: an effective theory of representation learning\"","tags":[],"title":"Some thoughts on Grokking","type":"post"},{"authors":["Ziming Liu","Ouail Kitouni","Niklas Nolte","Eric Michaud","Mike Williams","Max Tegmark"],"categories":null,"content":" ","date":1659830400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1659830400,"objectID":"484c1ef3ac07d05cb724961ee4079f5f","permalink":"https://okitouni.github.io/publication/grok/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/grok/","section":"publication","summary":"We develop an effective theory to understand transformer architectures’ ability to generalize on arithmetic datasets. The theory links generalization to a particular structured representation of the embeddings and predicts a range of phenomena associated with *grokking*, or delayed generalization. Moreover, we show that grokking is one of four different phases of learning and can be avoided with proper hyper-parameter tuning. To appear in NeurIPS2022.","tags":["Deep Learning"],"title":"Towards Understanding Grokking: An Effective Theory of Representation Learning","type":"publication"},{"authors":null,"categories":null,"content":" ","date":1649768400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649768400,"objectID":"4b90f08c3460fdd0a1cbbb9df8d0da45","permalink":"https://okitouni.github.io/talk/aps2022/","publishdate":"2022-04-12T13:00:00Z","relpermalink":"/talk/aps2022/","section":"talk","summary":" ","tags":null,"title":"Robust and Monotonic Neural Networks","type":"talk"},{"authors":["Ouail Kitouni","Niklas Nolte","Mike Williams"],"categories":null,"content":" ","date":1628294400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1628294400,"objectID":"470a94658e9e499585b83ece4193107f","permalink":"https://okitouni.github.io/publication/lipnn/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/lipnn/","section":"publication","summary":"We develop a novel architecture with an exact bound on its Lipschitz constant and which can be made monotonic in any subset of its features. This inductive bias is especially important for fairness and interpretability considerations.","tags":["Deep Learning"],"title":"Robust and Provably Monotonic Networks","type":"publication"},{"authors":["Ouail Kitouni"],"categories":null,"content":"","date":1596758400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596758400,"objectID":"44f95108a44ad2fe92adc9955d9bbdc5","permalink":"https://okitouni.github.io/random/bell/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/random/bell/","section":"random","summary":"Entanglement is one of the most fundamental aspects of quantum mechanics. It gives rise to phenomena that cannot be explained classically. In this work, we will demonstrate the non-locality of quantum mechanics through the violation of Bell’s Inequality.","tags":["Quantum Mechanics"],"title":"Bell Inequality Experiment","type":"random"},{"authors":["Ouail Kitouni","Benjamin Nachman","Constantin Weisser","Mike Williams"],"categories":null,"content":" ","date":1596758400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596758400,"objectID":"c2bb6418b262d2480392ae6ba0d2204a","permalink":"https://okitouni.github.io/publication/mode/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/mode/","section":"publication","summary":"Moment Decorrelation (MoDe) is a tool that can enforce decorrelation between some nuisance parameter (or protected attribute in ML fairness lingo) and the response of some model with gradient-based optimization (a neural network for example.) It can force trained models to have the same response across different values of the protected attribute but it can also go beyond simple decorrelation. For example, MoDe can constrain the response function to be linear or quadratic in the protected attribute.","tags":["Deep Learning"],"title":"Controlling Classifier Bias with Moment Decomposition: A Method to Enhance Searches for Resonances","type":"publication"}]