[{"authors":null,"categories":null,"content":"[wa-ill kitoonee] 🔊\nI am interested in the science of deep learning. Recently, I\u0026rsquo;ve been very excited about topics like reasoning, multi-modal foundation models, safe and scalable deep learning. During my time at Microsoft Research, I worked on developing a knowledge base generative model towards a knowledge-augmented LLM approach to improving interpretability and limit hallucination.\n","date":1701907200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1701907200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://okitouni.github.io/author/ouail-kitouni/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/ouail-kitouni/","section":"authors","summary":"[wa-ill kitoonee] 🔊\nI am interested in the science of deep learning. Recently, I\u0026rsquo;ve been very excited about topics like reasoning, multi-modal foundation models, safe and scalable deep learning. During my time at Microsoft Research, I worked on developing a knowledge base generative model towards a knowledge-augmented LLM approach to improving interpretability and limit hallucination.","tags":null,"title":"Ouail Kitouni","type":"authors"},{"authors":["Hansley Narasiah","Ouail Kitouni","Andrea Scorsoglio","Bernd Sturdza","Shawn Hatcher","Dolores Garcia","Matt Kusner"],"categories":null,"content":" ","date":1701907200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701907200,"objectID":"b2cc9d1bfd55a9f1ccc072e1c159357a","permalink":"https://okitouni.github.io/publication/csp/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/csp/","section":"publication","summary":"We use Bayesian Optimization to improve dry cooling systems for concentrated solar power plants, making them more cost-competitive.","tags":["Bayesian Optimization","ML for Physical Sciences"],"title":"ML Optimization for Concentrated Solar Power Plants","type":"publication"},{"authors":["Ouail Kitouni","Niklas Nolte","James Hensman","Bhaskar Mitra"],"categories":null,"content":" ","date":1693526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1693526400,"objectID":"3a0aa8f6369662d6e65259186769350e","permalink":"https://okitouni.github.io/publication/kbgen/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/kbgen/","section":"publication","summary":"KBFormer is a generative framework that can handle various data types, from numbers to complex hierarchical types. This model excels in tasks like populating missing data and is especially proficient at predicting numerical values. Its potential extends to augmenting language models for better information retrieval and knowledge manipulation.","tags":["Deep Learning"],"title":"KBFormer: Diffusion over Structured Entities","type":"publication"},{"authors":["Ouail Kitouni"],"categories":[],"content":"NuCLR can accurately predict many nuclear properties and generalizes well across tasks, indicating it has learned fundamental physics. Initial investigation reveals NuCLR has acquired some basic nuclear theory, including the Pauli Exclusion principle and magic numbers. Furthermore, NuCLR exhibits intriguing spiral embeddings for protons and neutrons. However, can we fully describe the mechanisms behind NuCLR\u0026rsquo;s performance? This project aims to fully reverse engineer the NuCLR algorithm to potentially derive new physics insights.\nThe work is still in progress, but code is available here.\nVisualization of the \u0026ldquo;nuclear\u0026rdquo; embeddings One-hot encoded representations of proton number, neutron number, and task are concatenated and input to the first layer. The visualization shows the first three principal components of all observed proton-neutron combinations, with the first two components as x-y axes and the third as color. Here\u0026rsquo;s a 3D version with the 4th principal component as color. In the visualizations below, you can choose which principal components to display as well as filter by neutron or proton numbers.\n","date":1687305600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687305600,"objectID":"cf3c1eca40fb8467c29f9befee48130d","permalink":"https://okitouni.github.io/post/nuclr/","publishdate":"2023-06-21T00:00:00Z","relpermalink":"/post/nuclr/","section":"post","summary":"A mechanistic interpretability effort that aims to reverse-engineer NuCLR, which accurately predicts nuclear properties and demonstrates an understanding of basic nuclear theory, to potentially derive new insights in nuclear physics.","tags":[],"title":"Reverse-engineering a Nuclear Physics model","type":"post"},{"authors":["Ouail Kitouni"],"categories":[],"content":" I don\u0026rsquo;t want to write a long-winded introduction on how the study of the steam engine, which started as an engineering discipline, flourished into a theory of thermodynamics. Or how physics specializes in breaking down complex systems into their simplest components to derive theories that explain the emergent behavior of the system. I assume you are already convinced that physics is a good place to seek inspiration.\nThis will be a series of short blog posts summarizing a sample of ideas and concepts in our understanding of deep learning. I will not claim that the material presented here will be THE physics approach to deep learning, as that could mean many things to different people. My definition of a physics approach to deep learning is one that adopts physicist-style thinking. This is not to say that the approach will be rigorous or even entirely correct. All we aim to achieve here is an intuitive understanding of some key components of deep learning. This intuition should generalize across different settings to a certain extent and make predictions we can test empirically. This approach is very pragmatic by its nature, and I plan on expending additional effort to ensure anyone can follow along. For those in the trenches, this would translate to adding a new tool to guide intuition in designing, training, and deploying models, and to everyone else, I hope you gain a fresh perspective and a new appreciation for familiar (or foreign) concepts in deep learning.\nSome topics I plan on talking about:\nGrokking, or generalization beyond overfitting. Lottery Ticket Hypothesis. Scaling Laws and infinite width limits. Topics in Mechanistic Interpretability. Miscellaneous topics in optimization via gradient descent in the Deep Learning setting. This includes topics like the role of normalization, adaptive optimization, implicit/explicit regularization, etc. ","date":1687305600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687305600,"objectID":"b9b2afe8ecb5ae61aba6ea72c5ed0f9c","permalink":"https://okitouni.github.io/post/physics-of-dl/","publishdate":"2023-06-21T00:00:00Z","relpermalink":"/post/physics-of-dl/","section":"post","summary":"Our theoretical understanding of deep learning is still in its infancy. And as we figure out what questions to even ask, perhaps it would be useful to draw inspiration from the field responsible for some of the most successful theories in science. This is a lazy repository of what we know about deep learning.","tags":[],"title":"The Physics of Deep Learning","type":"post"},{"authors":["Ouail Kitouni","Niklas Nolte","Sokratis Trifinopoulos","Subhash Kantamneni","Mike Williams"],"categories":null,"content":" ","date":1685577600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1685577600,"objectID":"e92b70db6e3b1d34fdbeb1e17136e7c7","permalink":"https://okitouni.github.io/publication/nuclr/","publishdate":"2023-06-01T00:00:00Z","relpermalink":"/publication/nuclr/","section":"publication","summary":"NuCLR is a state-of-the-art deep learning model that accurately predicts nuclear observables and reveals crucial aspects of the nuclear shell model, offering insights into nuclear theory and opening the doors to various applications in astrophysics.","tags":["Deep Learning","Nuclear Physics"],"title":"NuCLR: Nuclear Co-Learned Representations","type":"publication"},{"authors":["Ouail Kitouni"],"categories":[],"content":" Neural Estimation Energy Mover\u0026rsquo;s Distance (NEEMo) Triangle and Ellipse Fit Fitting the target distribution (green) with an ellipse and a triangle (red) using the NEEMo algorithm.\nMulti-circle Fit Now we fit the target distribution (green) with three circles (red). ","date":1668729600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668729600,"objectID":"c688edf0ac35fbf1048527274d94a63d","permalink":"https://okitouni.github.io/post/animations/","publishdate":"2022-11-18T00:00:00Z","relpermalink":"/post/animations/","section":"post","summary":"A collection of visualization I made in various projects","tags":[],"title":"Misc. Animations and Visualizations","type":"post"},{"authors":["Ouail Kitouni","Niklas Nolte","Mike Williams"],"categories":null,"content":" ","date":1663718400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1663718400,"objectID":"c6d8f170819df0005318e403a521a22f","permalink":"https://okitouni.github.io/publication/neemo/","publishdate":"2022-09-21T00:00:00Z","relpermalink":"/publication/neemo/","section":"publication","summary":"NEEMo is a technique to fit arbitrary geometries to an arbitrary collection of points. Much like WGANs, it relies on the neural estimation of the Wasserstein metric through the KR dual formulation.","tags":["Deep Learning","Particle Physics"],"title":"NEEMo: Geometric Fitting using Neural Estimation of the Energy Mover's Distance","type":"publication"},{"authors":["Ouail Kitouni"],"categories":[],"content":" Grokking, the phenomenon by which some models generalize well after overfitting, was very intriguing when it was first observed. In this post, I will try to summarize some observations I made while trying to understand this phenomenon.\nFormation of structure (animated) The example below is an animation of the first Figure from our Grokking paper. We visualize the first two principal components of the embeddings of a transformer model trained on the task of addition modulo 59. An interesting feature is that generalization seems to coincide with learning an ordered representation of the embeddings around a circle (much like how a human would reason about modular addition). What is more intriguing is that this \u0026ldquo;perfect\u0026rdquo; representation actually exists very early (up to local ordering of embeddings). In the animation below, the axes are fixed to the first two principal components at the end of training. This seems to suggest that the network picks a good representation at initialization and prunes away the noise throughout training. As one might expect, this \u0026ldquo;lottery ticket\u0026rdquo; found at initialization gets better (i.e., closer to the perfect representation) the wider the model dimension. N.b., it is not necessary to get a perfect ordering in the first two principal components for generalization to occur. In some cases, the circle can be \u0026ldquo;spread out\u0026rdquo; over many components at different frequencies. For instance, the circle can be ordered modulo 2 in the first two principal components such that there are two degenerate circles on top of each other. But this multiplicity is broken in lower principal components.\nSimple Grokking If you\u0026rsquo;re interested in playing around with Grokking, I made a very simple implementation that shows a clear delay in generalization. The task is modular addition. The model is a simple 2-layer MLP that takes in two learnable embeddings of dimension hidden_dim=128 concatenated together. Each embedding represents an integer and the target is their sum modulo 53. The code can be found here: https://github.com/okitouni/simple-grokking To undo Grokking (i.e. generalize earlier) we can simply increase the amount of weight decay used (from 0.03 to 5) and get the following results: ","date":1663718400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1663718400,"objectID":"c55acc8b759e9962841e40246aca1fc1","permalink":"https://okitouni.github.io/post/grokking-thoughts/","publishdate":"2022-09-21T00:00:00Z","relpermalink":"/post/grokking-thoughts/","section":"post","summary":"A short blog post on \"Towards Understanding Grokking: an effective theory of representation learning\"","tags":[],"title":"Some thoughts on Grokking","type":"post"},{"authors":null,"categories":null,"content":"Table of Contents Lipschitz Monotonic Networks Installation Requirements Usage Examples Monotonicity Robustness Lipschitz NNs can describe arbitrarily complex boundaries Lipschitz Monotonic Networks Implementation of Lipschitz Monotonic Networks, from the ICLR 2023 Submission: https://openreview.net/pdf?id=w2P7fMy_RH and the repo is on github.\nThe code here allows one to apply various weight constraints on torch.nn.Linear layers through the kind keyword. Here are the available weight norms:\n\u0026quot;one\u0026quot;, # |W|_1 constraint \u0026quot;inf\u0026quot;, # |W|_inf constraint \u0026quot;one-inf\u0026quot;, # |W|_1,inf constraint \u0026quot;two-inf\u0026quot;, # |W|_2,inf constraint Installation pip conda pip install monotonicnetworks\nconda install -c conda-forge monotonicnetworks\nNote that the package used to be called monotonenorm and was renamed to monotonicnetworks on 2023-07-15. The old package is still available on PyPI and conda-forge, but will not be updated.\n(deprecated) pip install monotonenorm\n(deprecated) conda install -c okitouni monotonenorm\nRequirements Make sure you have the following packages installed:\ntorch (required) matplotlib (optional, for plotting examples) tqdm (optional, to run the examples with a progress bar) Usage Here\u0026rsquo;s an example showing two ways to create a Lipschitz-constrained linear layer.\nfrom torch import nn import monotonicnetworks as lmn linear_by_norming = lmn.direct_norm(nn.Linear(10, 10), kind=\u0026quot;one-inf\u0026quot;) # |W|_1,inf constraint linear_native = lmn.LipschitzLinear(10, 10, kind=\u0026quot;one-inf\u0026quot;) # |W|_1,inf constraint The function lmn.direct_norm can apply various weight constraints on torch.nn.Linear layers through the kind keyword and return a Lipschitz-constrained linear layer. Alternatively, the code in montonenorm/LipschitzMonotonicNetwork.py contains several classes that can be used to create Lipschitz and Monotonic Layers directly.\nThe LipschitzLinear class is a linear layer with a Lipschitz constraint on its weights.\nThe MonotonicLayer class is a linear layer with a Lipschitz constraint on its weights and monotonicity constraints that can be specified for each input dimension, or for each input-output pair. For instance, suppose we want to model a 2 input x 3 output linear layer where the first output is monotonically increasing in the first input ([1, 0]), the second output is monotonically decreasing in the second input ([0, -1]), and the third output is monotonically increasing in the first input and monotonically decreasing in the second input ([1, -1]). We can do this by specifying the monotonicity constraints as follows:\nimport monotonicnetworks as lmn linear = lmn.MonotonicLayer(2, 3, monotonic_constraints=[[1, 0], [0, 1], [1, -1]]) Using a 1D tensor for the constraint assumes that they are the same for each output dimension. By default, the code assumes all outputs are monotonically increasing with all inputs.\nThe MonotonicWrapper class is a wrapper around a module with a Lipschitz constant. It adds a term to the output of the module which enforces monotonicity constraints given by monotonic_constraints. The class returns a module that is monotonic and Lipschitz with constant lipschitz_const. This is the preferred way to create a monotonic network. Example: from torch import nn import monotonicnetworks as lmn lip_nn = nn.Sequential( lmn.LipschitzLinear(2, 32, kind=\u0026quot;one-inf\u0026quot;), lmn.GroupSort(2), lmn.LipschitzLinear(32, 2, kind=\u0026quot;inf\u0026quot;), ) monotonic_nn = lmn.MonotonicWrapper(lip_nn, monotonic_constraints=[1,0]) # first input increasing, no monotonicity constraints on second input Note that one can stack monotonic modules.\nThe SigmaNet class is a deprecated class that is equivalent to the MonotonicWrapper class.\nThe RMSNorm class is a class that implements the RMSNorm normalization layer. It can help when training a model with many Lipschitz-constrained layers.\nExamples Check out the Examples directory for more details. Specifically, Examples/flower.py shows how to train a Lipschitz Monotonic Network to regress on a complex decision boundary in 2D (under Lipschitz NNs can describe arbitrarily complex boundaries), and Examples/Examples_paper.ipynb for the code used to make the plots under Monotonicity and Robustness.\nMonotonicity We will make a simple toy regression model to fit the following 1D function $$f(x) = log(x) + \\epsilon(x)$$ where $\\epsilon(x)$ is a Gaussian noise term whose variance is linearly increasing in x. In this toy model, we will assume that we have good reason to believe that the function we are trying to fit is monotonic (despite non-monotnic behavior of the noise). For example, we are building a trigger algorithm to discriminate between signal and background events. Rarer events are more likely to be signal and thus we should employ a network that is monotonic in some \u0026ldquo;rareness\u0026rdquo; feature. Another example could be a hiring classifier where (all else equal) higher school grades should imply better chances of being hired.\nTraining a monotonic NN and an unconstrained NN on the purple points and evaluating the networks on a uniform grid gives the following result:\nRobustness Now we will make a different toy model with one noisy data point. This will show that the Lipschitz continuous network is more robust against outliers than an unconstrained network because its gradient with respect to the input is bounded between -1 and 1. Additionally, it is more robust against adversarial attacks/data corruption for the same reason.\nLipschitz NNs can describe arbitrarily complex boundaries GroupSort weight-constrained Neural Networks are universal approximators of Lipschitz continuous functions. Furthermore, they can describe arbitrarily complex decision boundaries in classification problems provided the proper objective function is used in training. In Examples\\flower.py we provide code to regress on an example \u0026ldquo;complex\u0026rdquo; decision boundary in 2D.\nHere are the contour lines of the resulting network (along with the training points in black).\n","date":1659830400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1659830400,"objectID":"a3cd734f82736351be24d2bede09507d","permalink":"https://okitouni.github.io/project/monotonic/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/project/monotonic/","section":"project","summary":"A small package to make neural networks monotonic in any subset of their inputs (this works for individual neurons, too!).","tags":["Deep Learning","Interpretability"],"title":"Monotonic Networks","type":"project"},{"authors":["Ouail Kitouni","Niklas Nolte","Mike Williams"],"categories":null,"content":" ","date":1659830400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1659830400,"objectID":"470a94658e9e499585b83ece4193107f","permalink":"https://okitouni.github.io/publication/lipnn/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/lipnn/","section":"publication","summary":"We develop a novel neural architecture with an exact bound on its Lipschitz constant. The model can be made monotonic in any subset of its features. This inductive bias is especially important for fairness and interpretability considerations.","tags":["Deep Learning"],"title":"Robust and Provably Monotonic Networks","type":"publication"},{"authors":["Ziming Liu","Ouail Kitouni","Niklas Nolte","Eric Michaud","Mike Williams","Max Tegmark"],"categories":null,"content":" ","date":165456e4,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":165456e4,"objectID":"484c1ef3ac07d05cb724961ee4079f5f","permalink":"https://okitouni.github.io/publication/grok/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/grok/","section":"publication","summary":"This study investigates *grokking*, a generalization phenomenon first observed in transformer models trained on arithmetic data, using microscopic and macroscopic analyses, revealing four learning phases and a “Goldilocks zone” for optimal representation learning, while emphasizing the value of physics-inspired tools in understanding deep learning.","tags":["Deep Learning"],"title":"Towards Understanding Grokking: An Effective Theory of Representation Learning","type":"publication"},{"authors":null,"categories":null,"content":" ","date":1649768400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649768400,"objectID":"4b90f08c3460fdd0a1cbbb9df8d0da45","permalink":"https://okitouni.github.io/talk/aps2022/","publishdate":"2022-04-12T13:00:00Z","relpermalink":"/talk/aps2022/","section":"talk","summary":" ","tags":null,"title":"Robust and Monotonic Neural Networks","type":"talk"},{"authors":null,"categories":null,"content":"Get the code here.\nMoDe Moment Decorrelation (MoDe) is a tool that can enforce decorrelation between some nuisance parameter (or protected attribute in ML fairness lingo) and the response of some model with gradient-based optimization (a neural network for example.) It can force trained models to have the same response across different values of the protected attribute but it can also go beyond simple decorrelation. For example, MoDe can constrain the response function to be linear or quadratic in the protected attribute. This can increase performance tremendously if the independence constraint is not necessary but instead one only cares about the \u0026ldquo;smoothness\u0026rdquo; of the dependence of the response on the protected attribute (which is a weaker constraint).\nFor more details please see our article at 2010.09745. (Please use INSPIRE-HEP for citations.)\nAn implementation is available in each of TensorFlow and PyTorch. The notebooks in examples/ illustrate how MoDe is used and show how one can obtain different response functions on a toy example and on a W-tagging dataset.\nInstallation pip install modeloss The PyTorch implementation requires PyTorch 1.6.0 or newer. The TensorFlow implementation requires TensorFlow 2.2.0 or newer.\nUsage For PyTorch:\nfrom modeloss.pytorch import MoDeLoss flatness_loss = MoDeLoss(order=0) loss = lambda pred,target,m,weights: lambd * flatness_loss(pred,target,m,weights)+\\ classification_loss(pred,target,weights) For TensorFlow, replace modeloss.pytorch above with modeloss.tf.\nExample This is a toy example (located in examples/ToyExampleTF.ipynb) in which the signal (samples with label 1) is localized near the value 0.2 of the protected attribute m. While backgrounds (samples with label 0) are uniform in this feature, we note that this bias is introduced into any naive classifer. Indeed, we see that an unconstrained classifier (in the sense that it has no additional fairness regularization) has a large false positive rate for backgrounds near m = 0.2. Here we show how different MoDe regularizations (MoDe[n]) mitigate this bias by flattening the false positive rate as a function of the protected attribute m into its n\u0026rsquo;th legendre decomposition (where n is the highest moment allowed; 0 is flat, 1 is linear, etc) .\nLeft: The false positive rate versus mass (m) for various models at signal efficiencies (or true positive rates (TPR)) ε = 80, 50, 20% (each set of 3 identically colored and stylized lines correspond to the same model but with selection thresholds chosen to achieve the 3 desired TPRs). The bottom panel shows that MoDe[1] and MoDe[2] completely overlap with the m-agnostic model for this simple example, which is expected because the optimal classifier here has linear dependence on mass (see paper). Right: ROC curves for MoDe[0], MoDe[1], and MoDe[2] compared to the m-agnostic model and a model with unconstrained mass dependence. As in the left panel, we see that MoDe[1], MoDe[2], and the m-agnostic ROC curves are nearly identical because the optimal classifier has linear mass dependence in this simple example. For more details see 2010.09745.\n","date":1628294400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1628294400,"objectID":"71231b266666b113e7d249631d75724b","permalink":"https://okitouni.github.io/project/mode/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/project/mode/","section":"project","summary":"A regularization to make neural networks' output independent from certain features.","tags":["Deep Learning","Interpretability"],"title":"MoDe: Controlling Classifier Bias","type":"project"},{"authors":["Ouail Kitouni"],"categories":null,"content":"","date":1596758400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596758400,"objectID":"a62f12591c3851e3ab0891da60e423da","permalink":"https://okitouni.github.io/project/bell/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/project/bell/","section":"project","summary":"An experiment to demonstrate the non-locality of quantum mechanics through the violation of Bell’s Inequality.","tags":["Quantum Mechanics"],"title":"Bell Inequality Experiment","type":"project"},{"authors":["Ouail Kitouni","Benjamin Nachman","Constantin Weisser","Mike Williams"],"categories":null,"content":" ","date":1596758400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596758400,"objectID":"c2bb6418b262d2480392ae6ba0d2204a","permalink":"https://okitouni.github.io/publication/mode/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/mode/","section":"publication","summary":"Moment Decorrelation (MoDe) is a tool designed to ensure that a model's output remains uncorrelated with certain parameters, commonly termed as protected attributes in fairness contexts. Beyond mere decorrelation, MoDe can even shape the response function to adopt linear or quadratic relationships with the protected attribute.","tags":["Deep Learning"],"title":"Controlling Classifier Bias with Moment Decomposition: A Method to Enhance Searches for Resonances","type":"publication"}]