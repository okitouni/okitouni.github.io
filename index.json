[{"authors":null,"categories":null,"content":"[wa-ill kitooney] (Davis pronounces it best) 🔊\nI am a Ph.D. student at MIT in the Institute for AI and Fundamental Interactions. During my Ph.D., I worked a lot in research at the interface between machine learning and physics including AI robustness, fairness, and interpretability with applications to physics. I am also interested in understanding the science of deep learning and AI foundations. Recently, I\u0026rsquo;ve been very excited about AI reasoning, multi-modal foundation models, safe and scalable deep learning. During my time at Microsoft Research, I worked on developing a knowledge base generative model towards a knowledge-augmented LLM approach to improving interpretability and limit hallucination.\n","date":1687305600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1687305600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://okitouni.github.io/author/ouail-kitouni/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/ouail-kitouni/","section":"authors","summary":"[wa-ill kitooney] (Davis pronounces it best) 🔊\nI am a Ph.D. student at MIT in the Institute for AI and Fundamental Interactions. During my Ph.D., I worked a lot in research at the interface between machine learning and physics including AI robustness, fairness, and interpretability with applications to physics.","tags":null,"title":"Ouail Kitouni","type":"authors"},{"authors":["Ouail Kitouni"],"categories":[],"content":"NuCLR can accurately predict many nuclear properties and generalizes well across tasks, indicating it has learned fundamental physics. Initial investigation reveals NuCLR has acquired some basic nuclear theory, including the Pauli Exclusion principle and magic numbers. Furthermore, NuCLR exhibits intriguing spiral embeddings for protons and neutrons. However, can we fully describe the mechanisms behind NuCLR\u0026rsquo;s performance? This project aims to fully reverse engineer the NuCLR algorithm to potentially derive new physics insights.\nThe work is still in progress, but code is available here.\nVisualization of the \u0026ldquo;nuclear\u0026rdquo; embeddings One-hot encoded representations of proton number, neutron number, and task are concatenated and input to the first layer. The visualization shows the first three principal components of all observed proton-neutron combinations, with the first two components as x-y axes and the third as color. Here\u0026rsquo;s a 3D version with the 4th principal component as color. In the visualizations below, you can choose which principal components to display as well as filter by neutron or proton numbers.\n","date":1687305600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687305600,"objectID":"cf3c1eca40fb8467c29f9befee48130d","permalink":"https://okitouni.github.io/post/nuclr/","publishdate":"2023-06-21T00:00:00Z","relpermalink":"/post/nuclr/","section":"post","summary":"A mechanistic interpretability effort that aims to reverse-engineer NuCLR, which accurately predicts nuclear properties and demonstrates an understanding of basic nuclear theory, to potentially derive new insights in nuclear physics.","tags":[],"title":"Reverse-engineering a Nuclear Physics model","type":"post"},{"authors":["Ouail Kitouni"],"categories":[],"content":" I don\u0026rsquo;t want to write a long-winded introduction on how the study of the steam engine, which started as an engineering discipline, flourished into a theory of thermodynamics. Or how physics specializes in breaking down complex systems into their simplest components to derive theories that explain the emergent behavior of the system. I assume you are already convinced that physics is a good place to seek inspiration.\nThis will be a series of short blog posts summarizing a sample of ideas and concepts in our understanding of deep learning. I will not claim that the material presented here will be THE physics approach to deep learning, as that could mean many things to different people. My definition of a physics approach to deep learning is one that adopts physicist-style thinking. This is not to say that the approach will be rigorous or even entirely correct. All we aim to achieve here is an intuitive understanding of some key components of deep learning. This intuition should generalize across different settings to a certain extent and make predictions we can test empirically. This approach is very pragmatic by its nature, and I plan on expending additional effort to ensure anyone can follow along. For those in the trenches, this would translate to adding a new tool to guide intuition in designing, training, and deploying models, and to everyone else, I hope you gain a fresh perspective and a new appreciation for familiar (or foreign) concepts in deep learning.\nSome topics I plan on talking about:\nGrokking, or generalization beyond overfitting. Lottery Ticket Hypothesis. Scaling Laws and infinite width limits. Topics in Mechanistic Interpretability. Miscellaneous topics in optimization via gradient descent in the Deep Learning setting. This includes topics like the role of normalization, adaptive optimization, implicit/explicit regularization, etc. ","date":1687305600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687305600,"objectID":"b9b2afe8ecb5ae61aba6ea72c5ed0f9c","permalink":"https://okitouni.github.io/post/physics-of-dl/","publishdate":"2023-06-21T00:00:00Z","relpermalink":"/post/physics-of-dl/","section":"post","summary":"Our theoretical understanding of deep learning is still in its infancy. And as we figure out what questions to even ask, perhaps it would be useful to draw inspiration from the field responsible for some of the most successful theories in science. This is a lazy repository of what we know about deep learning.","tags":[],"title":"The Physics of Deep Learning","type":"post"},{"authors":["Ouail Kitouni","Niklas Nolte","Sokratis Trifinopoulos","Subhash Kantamneni","Mike Williams"],"categories":null,"content":" ","date":1685577600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1685577600,"objectID":"e92b70db6e3b1d34fdbeb1e17136e7c7","permalink":"https://okitouni.github.io/publication/nuclr/","publishdate":"2023-06-01T00:00:00Z","relpermalink":"/publication/nuclr/","section":"publication","summary":"NuCLR is a state-of-the-art deep learning model that accurately predicts nuclear observables and reveals crucial aspects of the nuclear shell model, offering insights into nuclear theory and opening the doors to various applications in astrophysics.","tags":["Deep Learning","Nuclear Physics"],"title":"NuCLR: Nuclear Co-Learned Representations","type":"publication"},{"authors":["Ouail Kitouni"],"categories":[],"content":" [This page is a work-in-progress.]\nNeural Estimation Energy Mover\u0026rsquo;s Distance (NEEMo) Triangle and Ellipse Fit Fitting the target distribution (green) with an ellipse and a triangle (red) using the NEEMo algorithm.\nMulti-circle Fit Now we fit the target distribution (green) with three circles (red). ","date":1668729600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668729600,"objectID":"c688edf0ac35fbf1048527274d94a63d","permalink":"https://okitouni.github.io/post/animations/","publishdate":"2022-11-18T00:00:00Z","relpermalink":"/post/animations/","section":"post","summary":"A collection of visualization I made in various projects","tags":[],"title":"Random Animations and Visualizations","type":"post"},{"authors":["Ouail Kitouni","Niklas Nolte","Mike Williams"],"categories":null,"content":" ","date":1663718400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1663718400,"objectID":"c6d8f170819df0005318e403a521a22f","permalink":"https://okitouni.github.io/publication/neemo/","publishdate":"2022-09-21T00:00:00Z","relpermalink":"/publication/neemo/","section":"publication","summary":"NEEMo is a technique to fit arbitrary geometries to an arbitrary collection of points. Much like WGANs, it relies on the neural estimation of the Wasserstein metric through the KR dual formulation.","tags":["Deep Learning","Particle Physics"],"title":"NEEMo: Geometric Fitting using Neural Estimation of the Energy Mover's Distance","type":"publication"},{"authors":["Ouail Kitouni"],"categories":[],"content":" [This page is a work-in-progress.]\nGrokking, the phenomenon by which some models generalize well after overfitting, was very intriguing when it was first observed. In this post, I will try to summarize some observations I made while trying to understand this phenomenon.\nFormation of structure (animated) The example below is an animation of the first Figure from our Grokking paper. We visualize the first two principal components of the embeddings of a transformer model trained on the task of addition modulo 59. An interesting feature is that generalization seems to coincide with learning an ordered representation of the embeddings around a circle (much like how a human would reason about modular addition). What is more intriguing is that this \u0026ldquo;perfect\u0026rdquo; representation actually exists very early (up to local ordering of embeddings). In the animation below, the axes are fixed to the first two principal components at the end of training. This seems to suggest that the network picks a good representation at initialization and prunes away the noise throughout training. As one might expect, this \u0026ldquo;lottery ticket\u0026rdquo; found at initialization gets better (i.e., closer to the perfect representation) the wider the model dimension. N.b., it is not necessary to get a perfect ordering in the first two principal components for generalization to occur. In some cases, the circle can be \u0026ldquo;spread out\u0026rdquo; over many components at different frequencies. For instance, the circle can be ordered modulo 2 in the first two principal components such that there are two degenerate circles on top of each other. But this multiplicity is broken in lower principal components.\nSimple Grokking If you\u0026rsquo;re interested in playing around with Grokking, I made a very simple implementation that shows a clear delay in generalization. The task is modular addition. The model is a simple 2-layer MLP that takes in two learnable embeddings of dimension hidden_dim=128 concatenated together. Each embedding represents an integer and the target is their sum modulo 53. The code can be found here: https://github.com/okitouni/simple-grokking To undo Grokking (i.e. generalize earlier) we can simply increase the amount of weight decay used (from 0.03 to 5) and get the following results: ","date":1663718400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1663718400,"objectID":"c55acc8b759e9962841e40246aca1fc1","permalink":"https://okitouni.github.io/post/grokking-thoughts/","publishdate":"2022-09-21T00:00:00Z","relpermalink":"/post/grokking-thoughts/","section":"post","summary":"A short blog post on \"Towards Understanding Grokking: an effective theory of representation learning\"","tags":[],"title":"Some thoughts on Grokking","type":"post"},{"authors":["Ziming Liu","Ouail Kitouni","Niklas Nolte","Eric Michaud","Mike Williams","Max Tegmark"],"categories":null,"content":" ","date":1659830400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1659830400,"objectID":"484c1ef3ac07d05cb724961ee4079f5f","permalink":"https://okitouni.github.io/publication/grok/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/grok/","section":"publication","summary":"This study investigates *grokking*, a generalization phenomenon first observed in transformer models trained on arithmetic data, using microscopic and macroscopic analyses, revealing four learning phases and a “Goldilocks zone” for optimal representation learning, while emphasizing the value of physics-inspired tools in understanding deep learning.","tags":["Deep Learning"],"title":"Towards Understanding Grokking: An Effective Theory of Representation Learning","type":"publication"},{"authors":null,"categories":null,"content":" ","date":1649768400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649768400,"objectID":"4b90f08c3460fdd0a1cbbb9df8d0da45","permalink":"https://okitouni.github.io/talk/aps2022/","publishdate":"2022-04-12T13:00:00Z","relpermalink":"/talk/aps2022/","section":"talk","summary":" ","tags":null,"title":"Robust and Monotonic Neural Networks","type":"talk"},{"authors":["Ouail Kitouni","Niklas Nolte","Mike Williams"],"categories":null,"content":" ","date":1628294400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1628294400,"objectID":"470a94658e9e499585b83ece4193107f","permalink":"https://okitouni.github.io/publication/lipnn/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/lipnn/","section":"publication","summary":"We develop a novel neural architecture with an exact bound on its Lipschitz constant. The model can be made monotonic in any subset of its features. This inductive bias is especially important for fairness and interpretability considerations.","tags":["Deep Learning"],"title":"Robust and Provably Monotonic Networks","type":"publication"},{"authors":["Ouail Kitouni"],"categories":null,"content":"","date":1596758400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596758400,"objectID":"44f95108a44ad2fe92adc9955d9bbdc5","permalink":"https://okitouni.github.io/random/bell/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/random/bell/","section":"random","summary":"Entanglement is one of the most fundamental aspects of quantum mechanics. It gives rise to phenomena that cannot be explained classically. In this work, we will demonstrate the non-locality of quantum mechanics through the violation of Bell’s Inequality.","tags":["Quantum Mechanics"],"title":"Bell Inequality Experiment","type":"random"},{"authors":["Ouail Kitouni","Benjamin Nachman","Constantin Weisser","Mike Williams"],"categories":null,"content":" ","date":1596758400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596758400,"objectID":"c2bb6418b262d2480392ae6ba0d2204a","permalink":"https://okitouni.github.io/publication/mode/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/mode/","section":"publication","summary":"Moment Decorrelation (MoDe) is a tool that can enforce decorrelation between some nuisance parameter (or protected attribute in ML fairness lingo) and the response of some model with gradient-based optimization (a neural network for example.) It can force trained models to have the same response across different values of the protected attribute but it can also go beyond simple decorrelation. For example, MoDe can constrain the response function to be linear or quadratic in the protected attribute.","tags":["Deep Learning"],"title":"Controlling Classifier Bias with Moment Decomposition: A Method to Enhance Searches for Resonances","type":"publication"}]